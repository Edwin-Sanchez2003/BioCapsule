"""
    BioCapsule
    Process Results

    This module takes the jons results generated from 
    the run_tests.py and gets more details and information
    from the data.
"""

import os
import copy
import tools
import math
import statistics

from sklearn.metrics import confusion_matrix


def main():
    get_mean_eer(results_dir="./MOBIO_extracted/test_results/")
    """
    append_info(
        results_dir="./MOBIO_extracted/test_results/",
        MOBIO_dataset_dir="./MOBIO_extracted/one_sec_intervals/",
    )  # end append_info
    """


# convenience function to get eer for a list of files
def get_mean_eer(results_dir: str) -> None:
    # get list of results file paths
    file_paths = tools.get_files_with_paths(input_dir=results_dir)
    for i, file_path in enumerate(file_paths):
        print(
            f"Starting file {i+1} of {len(file_paths)} | file: {os.path.basename(file_path)}"
        )
        get_comparable_results(file_path=file_path)


# this function gets results that allow us to make
# a table comparable to the paper:
# LIGHTWEIGHT AND SECURE FACE-BASED ACTIVE AUTHENTICATION FOR MOBILE USERS
# (KEYKHAIE AND PIERRE, 2023)
def get_comparable_results(file_path: str) -> None:
    """
    Gets the EER per location for the MOBIO
    dataset, as well as the standard deviation.

    WRITES BACK TO THE GIVEN FILE (with appended information)!

    Inputs:
    -------
    file_path : str
        The test json file to calculate results for. Assumed
        to be in the format generated by run_tests.py.
    """
    # load the given results json file
    results = tools.load_json_file(file_path=file_path)

    # the list of MOBIO locations
    mobio_locations = [
        "but",
        "idiap",
        "lia",
        "uman",
        "unis",
        "uoulu",
    ]  # end mobio locations

    # loop over locations, getting information per location
    test_mean_eer_list = []
    for loc in mobio_locations:
        print(f"Location: {loc}")
        # get subjects at this location
        subjects = get_subject_dicts_at_loc(results=results, location=loc)

        # loop over subjects at this location,
        # getting info to calculate EER,
        # and then calculating the EER.
        loc_eer_list = []
        for subject in subjects:
            # get the ground_truth labels and
            # predicted labels for each test
            # for a given subject
            gt_labels, pred_probs = get_labels_preds_from_subj(subject=subject)

            # using the labels and predicted probabilities,
            # calculate what the eer should be
            far, frr, threshold = calc_eer(
                gt_labels=gt_labels, pred_probs=pred_probs
            )  # end calc_eer

            eer = far  # far, frr are close -> they are at the eer

            # write the eer & test threshold found back to the dict
            subject["test_metrics"] = {}
            subject["test_metrics"]["test_threshold"] = threshold
            subject["test_metrics"]["eer"] = eer
            subject["test_metrics"]["far"] = far
            subject["test_metrics"]["frr"] = frr

            print(
                f"Subject: {subject['subject_id']} | eer: {eer} | Threshold: {threshold}"
            )

            # accumulate eer for this location
            loc_eer_list.append(eer)

        # once we get the summed eer for this location,
        # divide by the number of subjects at this location
        # to get mean eer
        loc_harmonic_mean_eer = statistics.harmonic_mean(loc_eer_list)
        loc_mean_eer = statistics.mean(loc_eer_list)
        print(
            f"Location: {loc} | Harmonic Mean EER: {loc_harmonic_mean_eer} | Mean EER: {loc_mean_eer}"
        )
        # store back into dict at results level
        results[loc] = {}
        results[loc]["harmonic_mean_eer"] = loc_harmonic_mean_eer
        results[loc]["mean_eer"] = loc_mean_eer
        results[loc]["std_dev"] = statistics.stdev(loc_eer_list)

        # add to test_mean_eer
        test_mean_eer_list.extend(copy.deepcopy(loc_eer_list))

    # once finished finding the mean_eer per location,
    # find test_mean_eer
    test_mean_eer = statistics.mean(test_mean_eer_list)
    test_harmonic_mean_eer = statistics.harmonic_mean(test_mean_eer_list)
    print(
        f"Test Mean EER: {test_mean_eer} | Test Harmonic Mean EER: {test_harmonic_mean_eer}"
    )
    results["mean_eer"] = test_mean_eer
    results["harmonic_mean_eer"] = test_harmonic_mean_eer
    results["std_dev"] = statistics.stdev(test_mean_eer_list)

    # write dict back to file
    tools.write_to_json(file_path=file_path, data=results)


# calculates the Equal Error Rate based on
# the ground truth labels and the predicted probabilities
def calc_eer(
    gt_labels: "list[int]",
    pred_probs: "list[float]",
    step_size: float = 0.001,
    precision_sig_figs: int = 2,
):
    # loop over possible thresholds
    # walk from threshold of 1.0 down to 0.0,
    # with the given step size (ex: 1.0, 0.99, 0.98 .. 0.01)
    # generate what the predicted class would be (0 or 1)
    # see if the calculated far & frr would be equal
    # (to some level of precision)
    for i in range(500, 0, -1):
        # calculate
        threshold = i * step_size
        # get classes using the threshold
        pred_labels = []
        for pred in pred_probs:
            # check the classification = 1 probability
            if pred >= threshold:
                pred_labels.append(1)
            else:
                pred_labels.append(0)
        # end for over predicted probabilities

        # get metrics based on thresh
        conf_matrix = confusion_matrix(
            y_true=gt_labels, y_pred=pred_labels, labels=[0, 1]
        )  # end confusion_matrix generation
        tn, fp, fn, tp = conf_matrix.ravel()
        far = get_far(fp=fp, tn=tn)
        frr = get_frr(fn=fn, tp=tp)

        # check if they are the same, to a precision level
        if math.isclose(far, frr, abs_tol=10**-precision_sig_figs):
            return (far, frr, threshold)
    raise Exception("Unable to find EER!!! (func: calc_eer())")


# False Acceptance Rate
def get_far(fp, tn) -> float:
    if (fp == 0) and (tn == 0):
        return None
    return float(fp / (fp + tn))


# False Rejection Rate
def get_frr(fn, tp) -> float:
    if (fn == 0) and (tp == 0):
        return None
    return float(fn / (fn + tp))


# get all the subjects at the given location
def get_subject_dicts_at_loc(results: dict, location: str) -> "list[dict]":
    subjects_list = []
    for subject in results["per_subject_results"]:
        if subject["location"] == location:
            subjects_list.append(subject)
    return subjects_list


# get the labels and predictions for each subject
# tested with respect to the subject who was the
# positive case for the classification
def get_labels_preds_from_subj(
    subject: dict,
) -> "tuple[list[int], list[float]]":
    all_labels = []
    all_preds = []
    # loop over every subject tested with
    # this subject's classifier
    for tested_subject in subject["test_results"]:
        # loop over every session tested
        # with this test subject
        for session_results in tested_subject["per_session_results"]:
            # add the labels and ground truth
            # to the lists
            all_labels.extend(
                copy.deepcopy(session_results["ground_truth_labels"])
            )  # end extend all_labels
            all_preds.extend(
                copy.deepcopy(session_results["predicted_probabilities_pos"])
            )  # end extend all_preds
    assert len(all_labels) == len(all_preds)
    return (all_labels, all_preds)


# convenience function for appending data
def append_info(results_dir: str, MOBIO_dataset_dir: str) -> None:
    # get results files in the results_dir
    file_paths = tools.get_files_with_paths(input_dir=results_dir)

    # loop over file_paths, adding location info to each file
    num_files = len(file_paths)
    for i, file_path in enumerate(file_paths):
        print(
            f"File {i+1} of {num_files} | File Name: {os.path.basename(file_path)}"
        )
        add_location_information(
            file_path=file_path,
            MOBIO_dataset_dir=MOBIO_dataset_dir,
        )  # end add_location_information


# reads in a test result file from run_tests.py and
# adds in location information for each subject
def add_location_information(file_path: str, MOBIO_dataset_dir: str) -> None:
    # gets a dict with MOBIO locations as keys
    # and subjects at that location as the values
    loc_subj_map = get_subj_at_loc(dataset_dir=MOBIO_dataset_dir)

    # open the given results file
    results = tools.load_json_file(file_path=file_path)

    # loop over each subject, adding in their location information
    num_subjects = len(results["per_subject_results"])
    for i, subject in enumerate(results["per_subject_results"]):
        print(f" Subject: {subject['subject_id']} | {i+1} of {num_subjects}")
        # loop over all the locations in MOBIO
        for loc, subj_list in loc_subj_map.items():
            # check if the subject is in the list for this location
            # if yes, add in the location information to the subject's
            # data and print out the info for visual confirmation
            if tools.is_in_list(subject["subject_id"], subj_list):
                subject["location"] = loc
                print(f"subject {subject['subject_id']} is at loc: {loc}")

    # write the appended results dict back into a json file
    tools.write_to_json(file_path=file_path, data=results)


# generates a dict where the key is one of
# the six MOBIO locations and the value is a
# list of subjects at that location
def get_subj_at_loc(dataset_dir: str) -> dict:
    # a list of locations from the dataset directory
    # MOBIO -> location dirs -> subject dirs
    mobio_locations = [
        "but",
        "idiap",
        "lia",
        "uman",
        "unis",
        "uoulu",
    ]  # end mobio locations

    loc_subj_map = {}
    for loc in mobio_locations:
        # get a list of subjects at a give location
        loc_subj_map[loc] = os.listdir(os.path.join(dataset_dir, loc))
    return loc_subj_map


if __name__ == "__main__":
    main()
